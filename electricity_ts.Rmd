---
title: "Electricity Time Series Project"
output:
  html_document:
    keep_md: yes

date: "2023-05-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
path <- "/Users/alisonwong/git/electricity-time-series"
knitr::opts_knit$set(root.dir = path)
setwd(path)
```

# Setup
```{r}
# Load Packages 
library(tidyverse)
library(forecast)
library(lmtest)
library(tseries)
```

# Read in data and perform data cleaning and wrangling 
```{r}
# Read data 
energy <- read.csv("energy-ca.csv")

# Subset data to select columns we need: Year, Month, State, Megawatts (sold)
energy_df <- subset(energy, select = c("X", "X.1", "X.2", "X.16"))
energy_df <- energy_df[-c(1,2),] # remove first two rows (unnecessary as these rows were the result of a small issue when importing the file)

# Add column names
colnames(energy_df) <- c("Year", "Month", "State", "Megawatts")
table(grepl("CA", energy_df$State)) # check the amount of times in which "CA" appears in the "X.2" column which corresponds to the respective state

# Subset data to select California 
energy_df <- energy_df[energy_df$State == "CA",]

# Get rid of empty rows
row.names(energy_df) <- NULL

# Check the amount of times in which the State column equals "CA". Since this number, 158, matches with the earlier number, we have support to say no important data was lost
table(grepl("CA", energy_df$State))
```

# Arrange data to create a time series object
```{r}
# Change to integers
energy_df$Year <- as.integer(energy_df$Year)
energy_df$Month <- as.integer(energy_df$Month)
energy_df$Megawatts <- as.integer(gsub(",", "", energy_df$Megawatts))

# Arrange energy_df in ascending order of time
energy_df <- energy_df %>%
  arrange(Year, Month)
```

To gauge how accurate our final ARMA model is, we will remove the 2022 data from our time series and use the 2010-2021 data to create our model and to predict the electricity sold in 2022. 
```{r}
# Remove the 2022 and 2023 year as only Jan and Feb data is present
energy2022 <- energy_df[133:158,]
energy_df1 <- energy_df[energy_df$Year != 2023 & energy_df$Year != 2022,] 

# Time series object
energy_ts <- ts(energy_df1[,4], start= 2010, frequency = 12)
energy2022 <- ts(energy2022[,4], start= 2021, frequency = 12)

# Divide data by a million to standardize units in a more perceivable way 
energy_ts <- energy_ts/(10^6)
energy2022 <- energy2022/(10^6)
```

# Exploratory Data Analysis (EDA)
### Analyzing the “smooth” component
```{r}
# Seasonal and Trend decomposition using Loess (locally weighted regression and scatter plot smoothing)
model <- stl(energy_ts, s.window = "periodic")

# Get m_t
m_t <- model$time.series[, "trend"]

# Plot time series data with yearly average
ts.plot(energy_ts,
        xlab = "Year",
        ylab = "Electricity Sold in Megawatts",
        main = "Electricity (Million Megawatts) Sold in California with Yearly Average")
lines(m_t, col = "red")
```
From the time series model, we can see that there is a seasonal trend in our electricity data with a peak in electricity sold in months around the summer of 2019.  

```{r}
# Overall trend
ts.plot(m_t,
        xlab = "Year",
        ylab = "Electricity Sold in Megawatts (Millions)",
        main = "Overall Trend")

# Detrend only
detrend <- energy_ts - m_t
```
The yearly average remains fairly constant from 2010 to 2022 with a slight drop from 2018 to 2020. 

Next, we will isolate the seasonality to get our residuals.
```{r}
# Plot new time series that clearly exposes seasonality
ts.plot(detrend,
        xlab = "Year",
        ylab = "Electricity Sold in Megawatts (Millions)",
        main = "Electricity Sold (Million Megawatts) after Detrending")

# Get seasonality
s_t <- model$time.series[, "seasonal"]
ts.plot(s_t, xlab = "Year", ylab = "Electricity Sold in Megawatts (Millions)", main = "Seasonality")
```

### Analyzing the residuals
```{r}
# Extract residuals
z_t <- energy_ts - m_t - s_t

# Plot time series, ACF, histogram, QQ-plot of white noise
checkresiduals(z_t)
qqnorm(z_t)
qqline(z_t, col = "red")
```
There are two outliers in our residuals as seen in the histogram that may affect our analysis: Aug 2018 and Sep 2018.

We will split the dataset into two (Jan 2010 - Jul 2018 and Oct 2018 - Dec 2022) to forecast and backcast the two outliers. We will then replace the outliers with the average of the 2 points from the forecasting/backcasting to ensure that the outliers do not affect our modelling and analysis. 
```{r}
# Jan 2010 - Jul 2018
df1 <- energy_df1[1:103,]
df1_ts <- ts(df1[,4], start= 2010, frequency = 12)
df1_ts <- df1_ts/(10^6)

# Oct 2018 - Dec 2022
df2 <- energy_df1[106:144,]
df2_ts <- ts(rev(df2[,4]), start= c(2018, 10), frequency = 12) # reversed order - have to reverse back 
df2_ts <- df2_ts/(10^6)

# Decompose both ts
decom_df1 <- stl(df1_ts, s.window = "periodic")
decom_df2 <- stl(df2_ts, s.window = "periodic")
```

First, we have to find the right values of ARIMA(p, d, q) that minimizes the AIC. We created a function that fits each p, q, d values to the residuals provided and finds the combination that exhibits the lowest AIC.
```{r}
# Function to find optimal p, q values
find_arima <- function(X, p_values, d_values, q_values) {
  
  aic_matrix <- matrix(NA, nrow = length(p_values), ncol = length(q_values))
  
  # Loop over all combinations of p, d, and q values
  for (i in 1:length(p_values)) {
    for (j in 1:length(d_values)) {
      for (k in 1:length(q_values)) {
        
        # Try fitting the ARMA model with current p, d, q values
        tryCatch({
          arma_model <- arima(X, order = c(p_values[i], d_values[j], q_values[k]))
          # Calculate the AIC value for the current model
          aic <- AIC(arma_model)
          # Store the AIC value in the matrix
          aic_matrix[i, k] <- aic
        }, error = function(e) {
          # If an error occurs, assign -1 to the matrix element
          aic_matrix[i, k] <- -1
        })
        
      }
    }
  }
  
  print(aic_matrix)
  
  # Find the indices of the minimum AIC value in the matrix
  min_indices <- which(aic_matrix == min(aic_matrix), arr.ind = TRUE)
  # Extract the corresponding p, d, q values with the minimum AIC
  best_p <- p_values[min_indices[, 1]]
  best_d <- d_values[min_indices[, 2]]
  best_q <- q_values[min_indices[, 2]]
  
  # Print the best p, d, q values
  cat("Best p:", best_p, "\n")
  cat("Best d:", best_d, "\n")
  cat("Best q:", best_q, "\n")
  cat("AIC:", min(aic_matrix, na.rm = TRUE), "\n")
}

# Define the range of values to consider for p, d, and q
p_values <- 0:6
d_values <- 0
q_values <- 0:6
```

Use the function created to find the most suitable ARMA(p,q) model
```{r}
find_arima(decom_df1$time.series[, "remainder"], p_values, d_values, q_values)
find_arima(decom_df2$time.series[, "remainder"], p_values, d_values, q_values)
arma1 <- arima(decom_df1$time.series[, "remainder"], order = c(4, 0, 5))
arma2 <- arima(decom_df2$time.series[, "remainder"], order = c(6, 0, 3))
```

Forecast and backcast the outlier residuals and find its average
```{r}
# Forecast the residuals w/ ARMA(5,6)
forecast_zt <- predict(arma1, n.ahead = 2)$pred

# Backcast the residuals w/ ARMA(5,6)
backcast_zt <- rev(predict(arma2, n.ahead = 2)$pred) # reverse to get Aug & Sept 2018 values

# Average
avg_zt <- (forecast_zt + backcast_zt)/2
avg_zt 
```

Replace outlier residuals in z_t  
```{r}
# Indices and values to replace
indices <- c(104, 105)
new_values <- c(avg_zt)
z_t[indices] <- new_values
```

Hypothesis testing for stationarity
```{r}
# Residual Diagnostics
checkresiduals(z_t)

# Augmented Dickey–Fuller test for stationarity
adf.test(z_t)
```
Since the p-value is smaller than 0.05 for the Augmented Dickey–Fuller test, we reject the null hypothesis and conclude that our residuals are stationary.

# Analyze the “rough” component

### Model Fitting

Now that we know that ARMA(4, 4) minimizes the AIC criterion, we will use those values to fit an ARMA model to our residuals.
```{r}
find_arima(z_t, p_values, d_values, q_values)

# Fit an ARMA model to the residuals
arma_model <- arima(z_t, order = c(4, 0, 4))
arma_model
```

### Residual Diagnostics - white noise, normality 
```{r}
# White Noise
acf(arma_model$residuals, na.action = na.pass, main = "ACF")
pacf(arma_model$residuals, na.action = na.pass, main = "PACF")

# Normality
qqnorm(arma_model$residuals, main = "Q-Q Plot") # Heavy tail
qqline(arma_model$residuals, col = "red")
```
The ACF and PACF shows that our residuals are not significant and falls under the normality assumption.

Since the residuals of our ARMA model is normal, we can use the Ljung-Box test to check for white noise where the null hypothesis states that the residuals are independent.
```{r}
Box.test(arma_model$residuals, type = "Ljung-Box")
```
As our p-value > 0.05, we cannot reject our null hypothesis so we can conclude that our residuals are white noise. This supports what we saw in our ACF and PACF plots where none of the lags exhibits significance, except for h = 0 in the ACF plot.

# Inference

### Interpretation

Find if the coefficients are significant
```{r}
coeftest(arma_model)
```
The P-value for the AR2 coefficient is larger than 0.05, which implies that it is not significant. The AR2 coefficient will be dropped from our final ARMA model.

$$X_t = 0.338X_{t-1} + 0.638X_{t-3} -0.730X_{t-4} + Z_t - 0.856Z_{t-1} - 0.215Z_{t-2} - 0.856Z_{t-3} + Z_{t-4}$$
# Forecasting future values of Xt's
```{r}
# Forecasting next year
xt_hat22 <- predict(arma_model, n.ahead = 12)

# Plot the time series with the 2022 forecast and 95% prediction intervals
plot(z_t, xlim = c(2010, 2023),
     main = "Time series white noise predictions from 2022 to 2023", ylab = "Xt")
points(xt_hat22$pred, type = "l", col = 2, lwd = 2)
points(xt_hat22$pred - 2*xt_hat22$se, type = "l", col = "blue", lty = 2, lwd = 0.2)
points(xt_hat22$pred + 2*xt_hat22$se, type = "l", col = "blue", lty = 2, lwd = 0.2)
legend("topleft", lty=1, bty = "n", col=c("red","black"), c("predicted values","actual values"))
```

Find prediction error of the Xt's
```{r}
# Get true Xt's in 2022
model2022 <- stl(energy2022, s.window = "periodic")
xt_22 <- model2022$time.series[, "remainder"]

# Get prediction error of Xt's in 2022
pred_error <- sum(xt_22 - xt_hat22$pred)^2/var(xt_22)
pred_error
var(xt_hat22$pred)
```
The prediction error of 0.206 is close to the variance of the predicted values of 0.227.

Now, we will forecast the predictions for electricity sales in 2022 (Yt).
```{r}
# To predict Yt, we have to first fit a polynomial to the trend to be able to predict the trend for the next year
t <- 1:length(m_t)
t2 <- t^2
t3 <- t^3
t4 <- t^4

# Fit model
trend_model <- lm(m_t ~ t + t2 + t3 + t4)
summary(trend_model)
g <- ts(predict(trend_model), start = 2010, frequency = 12)

# Plot 4th degree polynomial on the trend line
plot(m_t, main = "Fit 4th-degree Polynomial to Trend")
points(g, type = "l", lwd = 1, col = 2)

# Predict the trend for the 2022 year by fitting the next 12 data points to the model
x <- 145:156
m2022 <- 21.63 + 0.0004515*(x^2) - 0.000007607*(x^3) + 0.00000003032*(x^4)
```
The adjusted R-squared has a high value of 92.56% for the 4th degree polynomial, which means 92.56% of the variability is explained by the 4th degree polynomial model. 

```{r}
# Add the decomposed portion back for 2022
Yt <- xt_hat22$pred + s_t[1:12] + m2022

# Plot
plot(energy_ts, xlim = c(2010, 2023),
     main = "Time series predictions of 2022", ylab = "Electricity Sales (Million Megawatts)")
points(energy2022, type = "l", lwd = 1)
points(Yt, type = "l", col = 2, lwd = 2)
legend("topleft", lty=1, bty = "n", col=c("red","black"), c("predicted values","actual values"))
```

Prediction Error for electricity sales of the 2022 year
```{r}
pred_error <- sum(energy2022 - Yt)^2/var(energy2022[13:24])
pred_error
var(Yt)
``` 

# Spectral Analysis

References:
STL function https://www.geo.fu-berlin.de/en/v/soga/Geodata-analysis/time-series-analysis/Seasonal-decompositon/STL-decomposition/index.html#:~:text=In%20R%20the%20stl(),loess%20window%20for%20seasonal%20extraction.
ADF test https://www.r-bloggers.com/2022/06/augmented-dickey-fuller-test-in-r/
https://www.statology.org/ljung-box-test/
---
title: "Electricity Time Series Project"
output: html_document
date: "2023-05-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

path <- "/Users/alisonwong/git/electricity-time-series"
knitr::opts_knit$set(root.dir = path)
setwd(path)
```

# Setup
```{r}
# Load Packages 
library(tidyverse)
library(forecast)
library(lmtest)
library(tseries)
```

# Read in data and perform data cleaning and wrangling 
```{r}
# Read data 
energy <- read.csv("energy-ca.csv")

# Subset data to select columns we need: Year, Month, State, Megawatts (sold)
energy_df <- subset(energy, select = c("X", "X.1", "X.2", "X.16"))
energy_df <- energy_df[-c(1,2),] # remove first two rows (unnecessary as these rows were the result of a small issue when importing the file)

# Add column names
colnames(energy_df) <- c("Year", "Month", "State", "Megawatts")
table(grepl("CA", energy_df$State)) # check the amount of times in which "CA" appears in the "X.2" column which corresponds to the respective state

# Subset data to select California 
energy_df <- energy_df[energy_df$State == "CA",]

# Get rid of empty rows
row.names(energy_df) <- NULL

# Check the amount of times in which the State column equals "CA". Since this number, 158, matches with the earlier number, we have support to say no important data was lost
table(grepl("CA", energy_df$State))
```

# Arrange data to create a time series object
```{r}
# Change to integers
energy_df$Year <- as.integer(energy_df$Year)
energy_df$Month <- as.integer(energy_df$Month)
energy_df$Megawatts <- as.integer(gsub(",", "", energy_df$Megawatts))

# Arrange energy_df in ascending order of time
energy_df <- energy_df %>%
  arrange(Year, Month)
head(energy_df)

# Remove the 2023 year as only Jan and Feb data is present
energy_df <- energy_df[energy_df$Year != 2023,]

# Time series objedct
energy_ts <- ts(energy_df[,4], start= 2010, frequency = 12)
```

# Exploratory Data Analysis (EDA)
### Analyzing the “smooth” component

First, we compute the yearly average $\hat m_j$ over all months a $\hat m_j=\frac{1}{12}\sum_{k=1}^{12}x_{j,k}$ and eliminate the yearly trend.

```{r}
# Get overall trend by moving average window of 12
decomp <- decompose(energy_ts, type = "additive")
m_t <- decomp$trend

ts.plot(energy_ts,
        xlab = "Year",
        ylab = "Electricity Sold in Megawatts",
        main = "Electricity (Megawatts) Sold in California with Yearly Average")
lines(m_t, col = "red")
```
From the time series model, we can see that there is a seasonal trend in our electricity data with a peak in electricity sold in months around the summer of 2019.  

```{r}
# Overall trend
ts.plot(m_t,
        xlab = "Year",
        ylab = "Electricity Sold in Megawatts",
        main = "Overall Trend")

# Detrend only
detrend <- energy_ts - m_t
```
The yearly average remains fairly constant from 2010 to 2022 with a slight drop from 2018 to 2020.

Next, we will isolate the seasonality to get our residuals.
```{r}
# Plot new time series that clearly exposes seasonality
ts.plot(detrend,
        xlab = "Year",
        ylab = "Electricity Sold in Megawatts",
        main = "Electricity Sold (Megawatts) after Detrending")

# Get seasonality
s_t <- decomp$seasonal
ts.plot(s_t, xlab = "Year", ylab = "Electricity Sold in Megawatts", main = "Seasonality")
```

### Analyzing the residuals
```{r}
# Decomposed time series by detrending and deseasonalizing
# Extract residuals
z_t <- energy_ts - m_t - s_t

# Plot time series, ACF, histogram, QQ-plot of white noise
checkresiduals(z_t)
qqnorm(z_t)
qqline(z_t, col = "red")
```
There are two outliers in our residuals as seen in the histogram that may affect our analysis: Aug 2018 and Sep 2018.

We will split the dataset into two (Jan 2010 - Jul 2018 and Oct 2018 - Dec 2022) to forecast and backcast the two outliers. We will then replace the outliers with the average of the 2 points from the forecasting/backcasting to ensure that the outliers do not affect our modelling and analysis. 
```{r}
# Jan 2010 - Jul 2018
df1 <- energy_df[1:103,]
df1_ts <- ts(df1[,4], start= 2010, frequency = 12)

# Oct 2018 - Dec 2022
df2 <- energy_df[106:156,]
df2_ts <- ts(rev(df2[,4]), start= c(2018, 10), frequency = 12) # reversed order - have to reverse back 

# Decompose both ts
decom_df1 <- decompose(df1_ts, type = "additive")
decom_df2 <- decompose(df2_ts, type = "additive")
```

First, we have to find the right values of ARIMA(p, d, q) that minimizes the AIC. We created a function that fits each p, q, d values to the residuals provided and finds the combination that exhibits the lowest AIC.
```{r}
find_arima <- function(residuals, p_values, d_values, q_values) {
  
  aic_matrix <- matrix(NA, nrow = length(p_values), ncol = length(q_values))
  
  # Loop over all combinations of p, d, and q values
  for (i in 1:length(p_values)) {
    for (j in 1:length(d_values)) {
      for (k in 1:length(q_values)) {
        # Fit the ARMA model with current p, d, q values
        arma_model <- arima(decom_df1$random, order = c(p_values[i], d_values[j], q_values[k]))
      
        # Calculate the AIC value for the current model
        aic <- AIC(arma_model)
      
        # Store the AIC value in the matrix
        aic_matrix[i, k] <- aic
      }
    }
  }
  
  print(aic_matrix)
  
  # Find the indices of the minimum AIC value in the matrix
  min_indices <- which(aic_matrix == min(aic_matrix), arr.ind = TRUE)

  # Extract the corresponding p, d, q values with the minimum AIC
  best_p <- p_values[min_indices[, 1]]
  best_d <- d_values[min_indices[, 2]]
  best_q <- q_values[min_indices[, 2]]
  
  # Print the best p, d, q values
  cat("Best p:", best_p, "\n")
  cat("Best d:", best_d, "\n")
  cat("Best q:", best_q, "\n")
  cat("AIC:", min(aic_matrix), "\n")
}

# Define the range of values to consider for p, d, and q
p_values <- 0:8
d_values <- 0
q_values <- 0:8
```

Use the function created to find the most suitable ARMA(p,q) model
```{r}
find_arima(decom_df1$random, p_values, d_values, q_values)
find_arima(decom_df2$random, p_values, d_values, q_values)

arma1 <- arima(decom_df1$random, order = c(4, 0, 4))
arma2 <- arima(decom_df2$random, order = c(4, 0, 4))
```

Forecast and backcast the outlier residuals and find its average
```{r}
# Forecast the residuals w/ ARMA(5,6)
forecast_zt <- predict(arma1, n.ahead = 2)$pred

# Backcast the residuals w/ ARMA(5,6)
backcast_zt <- rev(predict(arma2, n.ahead = 2)$pred) # reverse to get Aug & Sept 2018 values

# Average
avg_zt <- (forecast_zt + backcast_zt)/2
avg_zt 
```

Replace outlier residuals in z_t  
```{r}
# Indices and vaalues to replace
indices <- c(104, 105)
new_values <- c(avg_zt)
z_t[indices] <- new_values
```

Hypothesis testing for stationarity
```{r}
checkresiduals(z_t)

# Augmented Dickey–Fuller test for stationarity
adf.test(z_t[7:150]) # since we used the two-sided MA, the first 6 and last 6 points are NA
```
Since the p-value is smaller than 0.05 for the Augmented Dickey–Fuller test, we reject the null hypothesis and conclude that our residuals are stationary.

# Analyze the “rough” component

### Model Fitting

Now that we know that ARMA(4, 4) minimizes the AIC criterion, we will use those values to fit an ARMA model to our residuals.
```{r}
find_arima(z_t, p_values, d_values, q_values)

# Fit an ARMA model to the residuals
arma_model <- arima(z_t, order = c(4, 0, 4))
arma_model
```

### Residual Diagnostics - white noise, normality 
```{r}
# White Noise
acf(arma_model$residuals, na.action=na.pass)
pacf(arma_model$residuals, na.action=na.pass)

# Normality
qqnorm(arma_model$residuals) # Heavy tail
qqline(arma_model$residuals, col = "red")
```
The ACF and PACF shows that our residuals are not significant and falls under the normality assumption.

Since the residuals of our ARMA model is normal, we can use the Ljung-Box test to check for white noise where the null hypothesis states that the residuals are independent.
```{r}
Box.test(arma_model$residuals, type = "Ljung-Box")
```
As our p-value > 0.05, we cannot reject our null hypothesis so we can conclude that our residuals are white noise. This supports what we saw in our ACF and PACF plots where none of the lags exhibits significance, except for h = 0 in the ACF plot.

# Inference

### Interpretation

Find if the coefficients are significant
```{r}
summary(arma_model)
coeftest(arma_model)
#AR - drop 2

# drop in formula
```
-What does the model imply?

FORMULA: 

# Forecasting future values

-Prediction
-Confidence interval 
-Comments
```{r}
zt_2023 <- predict(arma_model, n.ahead = 12)

# Plot the time series (with 2022) with the 2022 forecast and 95% prediction intervals
plot(z_t, xlim = c(2010, 2023),
     main = "Time series predictions from 2022 to 2023")

points(zt_2023$pred, type = "l", col = 2, lwd = 2)
points(zt_2023$pred - 2*zt_2023$se, type = "l", col = "blue", lty = 2, lwd = 0.2)
points(zt_2023$pred + 2*zt_2023$se, type = "l", col = "blue", lty = 2, lwd = 0.2)
legend("topleft", lty=1, bty = "n", col=c("red","black"), c("predicted values","actual values"))
```


```{r}
Yt <- zt_2023$pred + s_t[1:12] + m_t[145:156]

plot(energy_ts, xlim = c(2010, 2024),
     main = "Time series predictions from 2022 to 2026")

points(Yt, type = "l", col = 2, lwd = 2)
points(zt_2023$pred - 2*zt_2023$se + s_t[1:12] + m_t[145:156], 
       type = "l", col = "blue", lty = 2, lwd = 0.2)
points(zt_2023$pred + 2*zt_2023$se + s_t[1:12] + m_t[145:156], 
       type = "l", col = "blue", lty = 2, lwd = 0.2)
legend("topleft", lty=1, bty = "n", col=c("red","black"), c("predicted values","actual values"))
# how to predict m_t and s_t???
```

```{r}
# Calculate prediction error?
#accuracy(predict_AR$pred, energy_ts[145:156])
``` 

# Spectral Analysis
```{r}
spectrum()


#spec <- spec.ar(z_t, plot = FALSE, na.action = na.pass)  # parametric estimation based on AR model
# The latter's order is estimated using the Yule-Walker equations, with
# order selected by AIC
#sun_np = spectrum(z_t, spans = c(5, 5), plot = FALSE, na.action = na.pass)  # nonparametric  
#plot(sun_ar$freq, sun_ar$spec, type = "l", log = "y", ylab = "spectrum", xlab = "frequency", 
#    bty = "l")
#lines(sun_np$freq, sun_np$spec, lty = 2)
#legend("topright", c("parametric", "nonparametric"), lty = 1:2, bty = "n")

# draw periodogram
# decompose data in spectral domain 
```

References:

ADF test https://www.r-bloggers.com/2022/06/augmented-dickey-fuller-test-in-r/
https://www.statology.org/ljung-box-test/

https://rpubs.com/RatherBit/90267
https://rpubs.com/JSHAH/481706
